{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import pickle\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seet for numpy\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FRAC = 0.1  # use 10 percent of authors for training\n",
    "TEST_FRAC = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"../out/reddit_chunked/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(folder_path + \"reddit_train_pandas_df_20240126_181755.pickle\", \"rb\") as f:\n",
    "    train_df_embeddings = pickle.load(f)\n",
    "\n",
    "with open(folder_path + \"reddit_test_pandas_df_20240126_181755.pickle\", \"rb\") as f:\n",
    "    test_df_embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(folder_path + \"reddit_train_embeddings_20240126_184325.pickle\", \"rb\") as f:\n",
    "    train_chunks = pickle.load(f)\n",
    "\n",
    "\n",
    "with open(folder_path + \"reddit_test_embeddings_20240126_184325.pickle\", \"rb\") as f:\n",
    "    test_chunks = pickle.load(f)\n",
    "\n",
    "\n",
    "all_chunks = train_chunks + test_chunks\n",
    "all_chunks = all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2embedding = {\n",
    "    elem[\"text\"]: elem[\"embedding\"] for elem in all_chunks\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = pd.DataFrame(all_chunks)[[\"metadata\", \"text\"]]\n",
    "df_total[\"author\"] = df_total[\"metadata\"].apply(lambda x: x[\"author\"])\n",
    "df = df_total\n",
    "\n",
    "unique_authors = df[\"author\"].unique()\n",
    "np.random.shuffle(unique_authors)\n",
    "\n",
    "train_authors = unique_authors[:int(len(unique_authors) * TRAIN_FRAC)]\n",
    "test_authors = unique_authors[int(len(unique_authors) * TRAIN_FRAC):]\n",
    "\n",
    "train_df = df[df[\"author\"].isin(train_authors)].sort_values(by=\"author\")\n",
    "test_df = df[df[\"author\"].isin(test_authors)].sort_values(by=\"author\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test out what happens if the same author can be in either set \n",
    "df = df.sample(frac=1, replace=False, random_state=42)\n",
    "train_df = df.iloc[:int(len(df_total) * TRAIN_FRAC)]\n",
    "test_df = df.iloc[int(len(df_total) * TRAIN_FRAC):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train_df: 1000\n",
      "Length of test_df: 9000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of train_df: {len(train_df)}\")\n",
    "print(f\"Length of test_df: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_up_same_author(df):\n",
    "    # permute entire dataframe \n",
    "    old_df = deepcopy(df).sort_values(by=\"author\")\n",
    "    new_df = deepcopy(df).sample(frac=1, replace=False)\n",
    "\n",
    "    # sort dataframe again by author to keep random order within author\n",
    "    new_df = new_df.sort_values(by=[\"author\"], kind = \"stable\")\n",
    "\n",
    "    # rename all columns in parnter dataframe to avoid confusion\n",
    "    new_df.columns = [str(col) + \"_partner\" for col in new_df.columns]\n",
    "    \n",
    "    colnames = list(old_df.columns) + list(new_df.columns)\n",
    "\n",
    "    # horizontally concatenate the two dataframes ignoring the index\n",
    "    new_df = pd.concat([old_df.reset_index(drop = True), new_df.reset_index(drop = True)], axis=1, ignore_index=True)\n",
    "    new_df.columns = colnames\n",
    "\n",
    "    assert new_df[\"author\"].equals(new_df[\"author_partner\"]) # check if all authors are the same\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "\n",
    "def pair_up_different_author(df, iterations=10):\n",
    "    df_idx = df[[\"author\"]]\n",
    "    df_idx[\"old_idx\"] = range(len(df_idx))\n",
    "\n",
    "    pair_idx_df_list = []\n",
    "    for _ in range(iterations):\n",
    "        shuffled_df = df_idx.sample(frac=1, replace = False).reset_index(drop=True)\n",
    "\n",
    "        shuffled_df.rename(columns={\"author\": \"author_partner\", \"old_idx\": \"old_idx_partner\"}, inplace=True)\n",
    "\n",
    "        paired_df = pd.concat([deepcopy(df_idx).reset_index(drop=True), deepcopy(shuffled_df).reset_index(drop=True),], axis=1)\n",
    "\n",
    "        pair_idx_df_list.append(paired_df)\n",
    "\n",
    "    pair_idx_df = pd.concat(pair_idx_df_list, axis=0, ignore_index=True)\n",
    "    \n",
    "    # remove rows where author is the same\n",
    "    pair_idx_df = pair_idx_df[pair_idx_df[\"author\"] != pair_idx_df[\"author_partner\"]]\n",
    "\n",
    "    # drop duplicates for old indices \n",
    "    pair_idx_df = pair_idx_df.drop_duplicates(subset=[\"old_idx\"])   \n",
    "    \n",
    "    assert len(pair_idx_df) == len(df)\n",
    " \n",
    "    # order rows of origal df according to old_idx\n",
    "    pair_df = df.iloc[pair_idx_df[\"old_idx_partner\"].values, :]\n",
    "    pair_df.columns = [str(col) + \"_partner\" for col in pair_df.columns]\n",
    "\n",
    "    # concat df and pair_df\n",
    "    pair_df = pd.concat([df.reset_index(drop=True), pair_df.reset_index(drop=True)], axis=1, ignore_index=False)\n",
    "\n",
    "\n",
    "    assert pair_df[\"author\"].equals(pair_df[\"author_partner\"]) == False # check if all authors are the same\n",
    "\n",
    "    return pair_df\n",
    "\n",
    "def pair_up_different_author_baseline(df):\n",
    "    # just pair up the dataframe with a random permutation of itself\n",
    "\n",
    "    # permute entire dataframe\n",
    "    new_df = deepcopy(df).sample(frac=1, replace=False)\n",
    "\n",
    "    # rename all columns in parnter dataframe to avoid confusion\n",
    "    new_df.columns = [str(col) + \"_partner\" for col in new_df.columns]\n",
    "\n",
    "    colnames = list(df.columns) + list(new_df.columns)\n",
    "\n",
    "    # horizontally concatenate the two dataframes ignoring the index\n",
    "    new_df = pd.concat([df.reset_index(drop = True), new_df.reset_index(drop = True)], axis=1, ignore_index=True)\n",
    "    new_df.columns = colnames\n",
    "\n",
    "    return new_df\n",
    "\n",
    "\n",
    "\n",
    "# Example usage with your dataframe 'df'\n",
    "# result_df = pair_up_different_author(df)\n",
    "\n",
    "\n",
    "def create_pair_classification_df(df, clean_columns=True):\n",
    "    # create dataframe with same author pairs\n",
    "    same_author_df = pair_up_same_author(df)\n",
    "    # create dataframe with different author pairs\n",
    "    different_author_df = pair_up_different_author(df)\n",
    "\n",
    "    colnames_same_author_df = list(same_author_df.columns)\n",
    "    colnames_different_partner_df = list(different_author_df.columns)\n",
    "\n",
    "    # make sure that the columns are all of type string \n",
    "    colnames_same_author_df = [str(col) for col in colnames_same_author_df]\n",
    "    colnames_different_partner_df = [str(col) for col in colnames_different_partner_df]\n",
    "\n",
    "    assert len(colnames_same_author_df) == len(colnames_different_partner_df)\n",
    "    assert all([colnames_different_partner_df[i] == colnames_different_partner_df[i] for i in range(len(colnames_same_author_df))])\n",
    "\n",
    "    # rename columns in partner dataframe to avoid confusion\n",
    "    \n",
    "    same_author_df.columns = colnames_same_author_df\n",
    "    different_author_df.columns = colnames_different_partner_df\n",
    "    \n",
    "    #same_author_df[\"label\"] = 1\n",
    "    #different_author_df[\"label\"] = 0\n",
    "\n",
    "    pair_classification_df = pd.concat([same_author_df, different_author_df], axis=0, ignore_index=True)\n",
    "    \n",
    "    pair_classification_df = pair_classification_df.sample(frac=1, replace=False)\n",
    "\n",
    "    pair_classification_df[\"label\"] = pair_classification_df[\"author\"] == pair_classification_df[\"author_partner\"]\n",
    "\n",
    "    if clean_columns:\n",
    "        pair_classification_df.drop(columns = [\n",
    "            \"author\",\n",
    "            \"author_partner\",\n",
    "            \"metadata\",\n",
    "            \"metadata_partner\",\n",
    "            \"text\",\n",
    "            \"text_partner\"\n",
    "        ], inplace=True)\n",
    "\n",
    "    \n",
    "    return pair_classification_df\n",
    "\n",
    "\n",
    "def create_pair_classification_df_upsample(df, clean_columns, sample_ratio = 1):\n",
    "    ### use the craete_pair_classification_df function to create a pair classification dataframe. But stack several of them on top of each other to upsample the different author pairs\n",
    "\n",
    "    classification_df_lis = []\n",
    "    for _ in tqdm(range(sample_ratio)):\n",
    "        classification_df_lis.append(create_pair_classification_df(deepcopy(df), clean_columns=clean_columns))\n",
    "\n",
    "    classification_df = pd.concat(classification_df_lis, axis=0, ignore_index=False)\n",
    "    classification_df = classification_df.sample(frac=1, replace=False)\n",
    "\n",
    "    return classification_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_embeddings_to_df(df, text2embedding):\n",
    "    embedding_list = [\n",
    "        text2embedding[text] for text in df[\"text\"]\n",
    "    ]\n",
    "    embedding_df = pd.DataFrame(embedding_list)\n",
    "\n",
    "    embedding_list_partner = [\n",
    "        text2embedding[text] for text in df[\"text_partner\"]\n",
    "    ]\n",
    "    embedding_df_partner = pd.DataFrame(embedding_list_partner)\n",
    "    embedding_df_partner.columns = [str(col) + \"_partner\" for col in embedding_df_partner.columns]\n",
    "\n",
    "    df = pd.concat([df.reset_index(drop=True), embedding_df.reset_index(drop=True), embedding_df_partner.reset_index(drop=True)], axis=1, ignore_index=False)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]C:\\Users\\arik_\\AppData\\Local\\Temp\\ipykernel_8816\\3408655535.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_idx[\"old_idx\"] = range(len(df_idx))\n",
      "C:\\Users\\arik_\\AppData\\Local\\Temp\\ipykernel_8816\\3408655535.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_idx[\"old_idx\"] = range(len(df_idx))\n",
      "C:\\Users\\arik_\\AppData\\Local\\Temp\\ipykernel_8816\\3408655535.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_idx[\"old_idx\"] = range(len(df_idx))\n",
      "100%|██████████| 3/3 [00:00<00:00, 46.46it/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arik_\\AppData\\Local\\Temp\\ipykernel_8816\\3408655535.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_idx[\"old_idx\"] = range(len(df_idx))\n",
      "C:\\Users\\arik_\\AppData\\Local\\Temp\\ipykernel_8816\\3408655535.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_idx[\"old_idx\"] = range(len(df_idx))\n",
      " 67%|██████▋   | 2/3 [00:00<00:00, 17.45it/s]C:\\Users\\arik_\\AppData\\Local\\Temp\\ipykernel_8816\\3408655535.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_idx[\"old_idx\"] = range(len(df_idx))\n",
      "100%|██████████| 3/3 [00:00<00:00, 17.16it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df_paired = create_pair_classification_df_upsample(train_df, clean_columns = False, sample_ratio = 3)\n",
    "test_df_paired = create_pair_classification_df_upsample(test_df, clean_columns = False, sample_ratio = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = add_embeddings_to_df(train_df_paired, text2embedding)\n",
    "test_df = add_embeddings_to_df(test_df_paired, text2embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.iloc[:, 6:]\n",
    "test_df = test_df.iloc[:, 6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arik_\\Documents\\Dokumente\\Studium\\Master\\DataSecurity\\EmbeddingAttacks\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20240127_123536\"\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20240127_123536\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22631\n",
      "CPU Count:          8\n",
      "Memory Avail:       2.54 GB / 15.71 GB (16.2%)\n",
      "Disk Space Avail:   11.88 GB / 474.72 GB (2.5%)\n",
      "===================================================\n",
      "Train Data Rows:    6000\n",
      "Train Data Columns: 3072\n",
      "Label Column:       label\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [False, True]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = True, class 0 = False\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2583.26 MB\n",
      "\tTrain Data (Original)  Memory Usage: 140.63 MB (5.4% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 5.4% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 3072 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 3072 | ['0', '1', '2', '3', '4', ...]\n",
      "\t48.6s = Fit runtime\n",
      "\t3072 features in original data used to generate 3072 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 140.63 MB (4.9% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 49.63s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 5400, Val Rows: 600\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 250.37s of the 250.32s of remaining time.\n",
      "\t0.41\t = Validation score   (accuracy)\n",
      "\t5.52s\t = Training   runtime\n",
      "\t0.64s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 243.21s of the 243.13s of remaining time.\n",
      "\t0.4883\t = Validation score   (accuracy)\n",
      "\t1.0s\t = Training   runtime\n",
      "\t0.27s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 240.8s of the 240.73s of remaining time.\n",
      "\t0.6117\t = Validation score   (accuracy)\n",
      "\t93.32s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 147.28s of the 147.19s of remaining time.\n",
      "\t0.6317\t = Validation score   (accuracy)\n",
      "\t68.93s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 78.21s of the 78.13s of remaining time.\n",
      "\t0.4767\t = Validation score   (accuracy)\n",
      "\t38.36s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 39.58s of the 39.49s of remaining time.\n",
      "\t0.485\t = Validation score   (accuracy)\n",
      "\t49.98s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 250.37s of the -11.5s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 1.0}\n",
      "\t0.6317\t = Validation score   (accuracy)\n",
      "\t0.48s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 312.45s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240127_123536\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<autogluon.tabular.predictor.predictor.TabularPredictor at 0x2bdab0edcc0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor = TabularPredictor(label='label')\n",
    "predictor.fit(train_df, time_limit=60*15, presets = 'medium_quality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      -0.035170\n",
       "1      -0.006340\n",
       "2      -0.033977\n",
       "3      -0.047606\n",
       "4      -0.006359\n",
       "          ...   \n",
       "5995   -0.047155\n",
       "5996   -0.015223\n",
       "5997   -0.031893\n",
       "5998   -0.038155\n",
       "5999   -0.014170\n",
       "Name: 1535_partner, Length: 6000, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>1526_partner</th>\n",
       "      <th>1527_partner</th>\n",
       "      <th>1528_partner</th>\n",
       "      <th>1529_partner</th>\n",
       "      <th>1530_partner</th>\n",
       "      <th>1531_partner</th>\n",
       "      <th>1532_partner</th>\n",
       "      <th>1533_partner</th>\n",
       "      <th>1534_partner</th>\n",
       "      <th>1535_partner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>0.006676</td>\n",
       "      <td>-0.021149</td>\n",
       "      <td>0.022263</td>\n",
       "      <td>-0.021941</td>\n",
       "      <td>0.006342</td>\n",
       "      <td>-0.017474</td>\n",
       "      <td>0.001129</td>\n",
       "      <td>-0.004604</td>\n",
       "      <td>0.004863</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003998</td>\n",
       "      <td>-0.002934</td>\n",
       "      <td>-0.007514</td>\n",
       "      <td>-0.031906</td>\n",
       "      <td>-0.014525</td>\n",
       "      <td>0.011988</td>\n",
       "      <td>-0.009024</td>\n",
       "      <td>-0.016225</td>\n",
       "      <td>-0.003482</td>\n",
       "      <td>-0.035170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>-0.014148</td>\n",
       "      <td>-0.005231</td>\n",
       "      <td>-0.000418</td>\n",
       "      <td>-0.019133</td>\n",
       "      <td>-0.034236</td>\n",
       "      <td>0.015659</td>\n",
       "      <td>-0.004724</td>\n",
       "      <td>-0.018378</td>\n",
       "      <td>0.014784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013752</td>\n",
       "      <td>0.026534</td>\n",
       "      <td>-0.006965</td>\n",
       "      <td>-0.025878</td>\n",
       "      <td>-0.017088</td>\n",
       "      <td>0.016391</td>\n",
       "      <td>0.008674</td>\n",
       "      <td>-0.012037</td>\n",
       "      <td>-0.021586</td>\n",
       "      <td>-0.006340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>0.001571</td>\n",
       "      <td>-0.002970</td>\n",
       "      <td>0.008940</td>\n",
       "      <td>-0.041818</td>\n",
       "      <td>-0.022341</td>\n",
       "      <td>0.016021</td>\n",
       "      <td>0.003361</td>\n",
       "      <td>-0.018672</td>\n",
       "      <td>-0.008262</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015367</td>\n",
       "      <td>-0.000937</td>\n",
       "      <td>-0.011893</td>\n",
       "      <td>-0.020796</td>\n",
       "      <td>-0.015569</td>\n",
       "      <td>0.014764</td>\n",
       "      <td>0.002307</td>\n",
       "      <td>-0.004279</td>\n",
       "      <td>0.025608</td>\n",
       "      <td>-0.033977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>-0.042519</td>\n",
       "      <td>-0.029029</td>\n",
       "      <td>0.006739</td>\n",
       "      <td>-0.002219</td>\n",
       "      <td>-0.025803</td>\n",
       "      <td>-0.016742</td>\n",
       "      <td>-0.016562</td>\n",
       "      <td>-0.023372</td>\n",
       "      <td>0.002672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002832</td>\n",
       "      <td>-0.006415</td>\n",
       "      <td>0.008959</td>\n",
       "      <td>-0.012771</td>\n",
       "      <td>0.013837</td>\n",
       "      <td>0.008128</td>\n",
       "      <td>-0.024967</td>\n",
       "      <td>0.003268</td>\n",
       "      <td>-0.001159</td>\n",
       "      <td>-0.047606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>-0.001586</td>\n",
       "      <td>0.015298</td>\n",
       "      <td>-0.020849</td>\n",
       "      <td>-0.018154</td>\n",
       "      <td>-0.005949</td>\n",
       "      <td>0.014555</td>\n",
       "      <td>-0.030013</td>\n",
       "      <td>-0.029402</td>\n",
       "      <td>-0.021620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015599</td>\n",
       "      <td>-0.016498</td>\n",
       "      <td>0.012098</td>\n",
       "      <td>-0.016253</td>\n",
       "      <td>-0.018814</td>\n",
       "      <td>0.019659</td>\n",
       "      <td>-0.000748</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>-0.028283</td>\n",
       "      <td>-0.006359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>True</td>\n",
       "      <td>-0.003460</td>\n",
       "      <td>0.002003</td>\n",
       "      <td>0.014805</td>\n",
       "      <td>-0.000302</td>\n",
       "      <td>-0.002699</td>\n",
       "      <td>0.014660</td>\n",
       "      <td>0.001945</td>\n",
       "      <td>-0.010135</td>\n",
       "      <td>-0.011398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001202</td>\n",
       "      <td>0.004714</td>\n",
       "      <td>-0.003008</td>\n",
       "      <td>-0.027706</td>\n",
       "      <td>-0.008507</td>\n",
       "      <td>-0.006443</td>\n",
       "      <td>0.001912</td>\n",
       "      <td>-0.004733</td>\n",
       "      <td>-0.013396</td>\n",
       "      <td>-0.047155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>True</td>\n",
       "      <td>0.016424</td>\n",
       "      <td>-0.019949</td>\n",
       "      <td>0.004093</td>\n",
       "      <td>-0.046820</td>\n",
       "      <td>-0.033572</td>\n",
       "      <td>0.007341</td>\n",
       "      <td>-0.007031</td>\n",
       "      <td>-0.011666</td>\n",
       "      <td>-0.014784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011453</td>\n",
       "      <td>-0.018839</td>\n",
       "      <td>0.023332</td>\n",
       "      <td>-0.008012</td>\n",
       "      <td>-0.028923</td>\n",
       "      <td>0.021111</td>\n",
       "      <td>-0.028071</td>\n",
       "      <td>0.003186</td>\n",
       "      <td>-0.008716</td>\n",
       "      <td>-0.015223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>True</td>\n",
       "      <td>-0.015440</td>\n",
       "      <td>-0.001745</td>\n",
       "      <td>-0.016699</td>\n",
       "      <td>-0.013188</td>\n",
       "      <td>-0.020614</td>\n",
       "      <td>0.013250</td>\n",
       "      <td>-0.045308</td>\n",
       "      <td>-0.029046</td>\n",
       "      <td>-0.013681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021805</td>\n",
       "      <td>0.002303</td>\n",
       "      <td>0.012785</td>\n",
       "      <td>-0.024337</td>\n",
       "      <td>-0.016467</td>\n",
       "      <td>0.002926</td>\n",
       "      <td>-0.027801</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.005616</td>\n",
       "      <td>-0.031893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>True</td>\n",
       "      <td>-0.005819</td>\n",
       "      <td>-0.016166</td>\n",
       "      <td>0.007109</td>\n",
       "      <td>-0.004758</td>\n",
       "      <td>-0.023752</td>\n",
       "      <td>0.004835</td>\n",
       "      <td>0.009489</td>\n",
       "      <td>-0.005928</td>\n",
       "      <td>-0.019753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024626</td>\n",
       "      <td>0.018540</td>\n",
       "      <td>-0.017250</td>\n",
       "      <td>-0.018056</td>\n",
       "      <td>-0.020058</td>\n",
       "      <td>0.009330</td>\n",
       "      <td>0.018486</td>\n",
       "      <td>-0.023779</td>\n",
       "      <td>-0.013321</td>\n",
       "      <td>-0.038155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>True</td>\n",
       "      <td>0.006754</td>\n",
       "      <td>-0.001064</td>\n",
       "      <td>0.004228</td>\n",
       "      <td>0.001846</td>\n",
       "      <td>-0.005375</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>-0.013603</td>\n",
       "      <td>-0.014757</td>\n",
       "      <td>-0.006034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003123</td>\n",
       "      <td>-0.000228</td>\n",
       "      <td>0.004355</td>\n",
       "      <td>-0.017023</td>\n",
       "      <td>-0.026115</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>-0.002932</td>\n",
       "      <td>0.002515</td>\n",
       "      <td>-0.020941</td>\n",
       "      <td>-0.014170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows × 3073 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label         0         1         2         3         4         5  \\\n",
       "0     False  0.006676 -0.021149  0.022263 -0.021941  0.006342 -0.017474   \n",
       "1      True -0.014148 -0.005231 -0.000418 -0.019133 -0.034236  0.015659   \n",
       "2      True  0.001571 -0.002970  0.008940 -0.041818 -0.022341  0.016021   \n",
       "3     False -0.042519 -0.029029  0.006739 -0.002219 -0.025803 -0.016742   \n",
       "4      True -0.001586  0.015298 -0.020849 -0.018154 -0.005949  0.014555   \n",
       "...     ...       ...       ...       ...       ...       ...       ...   \n",
       "5995   True -0.003460  0.002003  0.014805 -0.000302 -0.002699  0.014660   \n",
       "5996   True  0.016424 -0.019949  0.004093 -0.046820 -0.033572  0.007341   \n",
       "5997   True -0.015440 -0.001745 -0.016699 -0.013188 -0.020614  0.013250   \n",
       "5998   True -0.005819 -0.016166  0.007109 -0.004758 -0.023752  0.004835   \n",
       "5999   True  0.006754 -0.001064  0.004228  0.001846 -0.005375  0.000815   \n",
       "\n",
       "             6         7         8  ...  1526_partner  1527_partner  \\\n",
       "0     0.001129 -0.004604  0.004863  ...     -0.003998     -0.002934   \n",
       "1    -0.004724 -0.018378  0.014784  ...      0.013752      0.026534   \n",
       "2     0.003361 -0.018672 -0.008262  ...     -0.015367     -0.000937   \n",
       "3    -0.016562 -0.023372  0.002672  ...      0.002832     -0.006415   \n",
       "4    -0.030013 -0.029402 -0.021620  ...      0.015599     -0.016498   \n",
       "...        ...       ...       ...  ...           ...           ...   \n",
       "5995  0.001945 -0.010135 -0.011398  ...      0.001202      0.004714   \n",
       "5996 -0.007031 -0.011666 -0.014784  ...      0.011453     -0.018839   \n",
       "5997 -0.045308 -0.029046 -0.013681  ...      0.021805      0.002303   \n",
       "5998  0.009489 -0.005928 -0.019753  ...      0.024626      0.018540   \n",
       "5999 -0.013603 -0.014757 -0.006034  ...      0.003123     -0.000228   \n",
       "\n",
       "      1528_partner  1529_partner  1530_partner  1531_partner  1532_partner  \\\n",
       "0        -0.007514     -0.031906     -0.014525      0.011988     -0.009024   \n",
       "1        -0.006965     -0.025878     -0.017088      0.016391      0.008674   \n",
       "2        -0.011893     -0.020796     -0.015569      0.014764      0.002307   \n",
       "3         0.008959     -0.012771      0.013837      0.008128     -0.024967   \n",
       "4         0.012098     -0.016253     -0.018814      0.019659     -0.000748   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "5995     -0.003008     -0.027706     -0.008507     -0.006443      0.001912   \n",
       "5996      0.023332     -0.008012     -0.028923      0.021111     -0.028071   \n",
       "5997      0.012785     -0.024337     -0.016467      0.002926     -0.027801   \n",
       "5998     -0.017250     -0.018056     -0.020058      0.009330      0.018486   \n",
       "5999      0.004355     -0.017023     -0.026115      0.000281     -0.002932   \n",
       "\n",
       "      1533_partner  1534_partner  1535_partner  \n",
       "0        -0.016225     -0.003482     -0.035170  \n",
       "1        -0.012037     -0.021586     -0.006340  \n",
       "2        -0.004279      0.025608     -0.033977  \n",
       "3         0.003268     -0.001159     -0.047606  \n",
       "4         0.000499     -0.028283     -0.006359  \n",
       "...            ...           ...           ...  \n",
       "5995     -0.004733     -0.013396     -0.047155  \n",
       "5996      0.003186     -0.008716     -0.015223  \n",
       "5997      0.003552      0.005616     -0.031893  \n",
       "5998     -0.023779     -0.013321     -0.038155  \n",
       "5999      0.002515     -0.020941     -0.014170  \n",
       "\n",
       "[6000 rows x 3073 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: accuracy on test data: 0.5435925925925926\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"accuracy\": 0.5435925925925926,\n",
      "    \"balanced_accuracy\": 0.5466298325216553,\n",
      "    \"mcc\": 0.09737473949373732,\n",
      "    \"roc_auc\": 0.5679744988469059,\n",
      "    \"f1\": 0.4731509191962377,\n",
      "    \"precision\": 0.5759562841530055,\n",
      "    \"recall\": 0.40148739343370216\n",
      "}\n",
      "Detailed (per-class) classification report:\n",
      "{\n",
      "    \"False\": {\n",
      "        \"precision\": 0.5257151070863878,\n",
      "        \"recall\": 0.6917722716096085,\n",
      "        \"f1-score\": 0.5974191440705652,\n",
      "        \"support\": 26435.0\n",
      "    },\n",
      "    \"True\": {\n",
      "        \"precision\": 0.5759562841530055,\n",
      "        \"recall\": 0.40148739343370216,\n",
      "        \"f1-score\": 0.4731509191962377,\n",
      "        \"support\": 27565.0\n",
      "    },\n",
      "    \"accuracy\": 0.5435925925925926,\n",
      "    \"macro avg\": {\n",
      "        \"precision\": 0.5508356956196967,\n",
      "        \"recall\": 0.5466298325216553,\n",
      "        \"f1-score\": 0.5352850316334015,\n",
      "        \"support\": 54000.0\n",
      "    },\n",
      "    \"weighted avg\": {\n",
      "        \"precision\": 0.5513613671945603,\n",
      "        \"recall\": 0.5435925925925926,\n",
      "        \"f1-score\": 0.5339848177990683,\n",
      "        \"support\": 54000.0\n",
      "    }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.5435925925925926, 'balanced_accuracy': 0.5466298325216553, 'mcc': 0.09737473949373732, 'roc_auc': 0.5679744988469059, 'f1': 0.4731509191962377, 'precision': 0.5759562841530055, 'recall': 0.40148739343370216, 'confusion_matrix':        False  True \n",
      "False  18287   8148\n",
      "True   16498  11067, 'classification_report': {'False': {'precision': 0.5257151070863878, 'recall': 0.6917722716096085, 'f1-score': 0.5974191440705652, 'support': 26435.0}, 'True': {'precision': 0.5759562841530055, 'recall': 0.40148739343370216, 'f1-score': 0.4731509191962377, 'support': 27565.0}, 'accuracy': 0.5435925925925926, 'macro avg': {'precision': 0.5508356956196967, 'recall': 0.5466298325216553, 'f1-score': 0.5352850316334015, 'support': 54000.0}, 'weighted avg': {'precision': 0.5513613671945603, 'recall': 0.5435925925925926, 'f1-score': 0.5339848177990683, 'support': 54000.0}}}\n"
     ]
    }
   ],
   "source": [
    "print(predictor.evaluate(test_df, silent=False, detailed_report =True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
