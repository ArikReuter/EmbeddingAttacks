{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYQJSKu1tOL-",
        "outputId": "5d79b3b1-1b40-47d1-aa3c-85c5b55035cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = \"/content/drive/MyDrive/EmbeddingsAttack/out/reddit_chunked/\""
      ],
      "metadata": {
        "id": "Aj867Fb5tWWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bB__WVwTtLJm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRaLv6VOtLJp"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kk1vE-qFtLJp"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QD5KGpvQtLJq"
      },
      "outputs": [],
      "source": [
        "TRAIN_FRAC = 0.8  # use 10 percent of authors for training\n",
        "TEST_FRAC = 0.2\n",
        "folder_path = \"/content/drive/MyDrive/EmbeddingsAttack/out/reddit_chunked/\"\n",
        "with open(folder_path + \"reddit_train_embeddings_20240126_181755.pickle\", \"rb\") as f:\n",
        "    train_chunks = pickle.load(f)\n",
        "\n",
        "with open(folder_path + \"reddit_test_embeddings_20240126_181755.pickle\", \"rb\") as f:\n",
        "    test_chunks = pickle.load(f)\n",
        "\n",
        "\n",
        "all_chunks = train_chunks + test_chunks\n",
        "all_chunks = all_chunks\n",
        "text2embedding = {\n",
        "    elem[\"text\"]: torch.tensor(elem[\"embedding\"]) for elem in all_chunks\n",
        "}\n",
        "text2hash = {\n",
        "    elem[\"text\"]: hash(elem[\"text\"]) for elem in all_chunks\n",
        "}\n",
        "hash2text = {\n",
        "    a:b for b,a in text2hash.items()\n",
        "}\n",
        "\n",
        "hash2embedding = {\n",
        "    text2hash[text]: text2embedding[text] for text in text2embedding\n",
        "}\n",
        "\n",
        "df_total = pd.DataFrame(all_chunks)[[\"metadata\", \"text\"]]\n",
        "df_total[\"author\"] = df_total[\"metadata\"].apply(lambda x: x['author'])\n",
        "df = df_total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUTWDopptLJr",
        "outputId": "0eadfeb6-b4dd-428b-c873-4e13185ec926"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-169-b415dc18d3c3>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"text_hash\"] = df[\"text\"].apply(lambda x: text2hash[x])\n"
          ]
        }
      ],
      "source": [
        "most_frequent_authors = df[\"author\"].value_counts().index[:10]\n",
        "df = df[df[\"author\"].isin(most_frequent_authors)]\n",
        "\n",
        "df[\"text_hash\"] = df[\"text\"].apply(lambda x: text2hash[x])\n",
        "df = df[[\"author\", \"text_hash\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBHKuj4EtLJs"
      },
      "outputs": [],
      "source": [
        "train_df = df.iloc[:int(len(df) * TRAIN_FRAC)]\n",
        "test_df = df.iloc[int(len(df) * TRAIN_FRAC):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDt8sxnAtLJt"
      },
      "outputs": [],
      "source": [
        "# create dataloader for pairs of texts, such that the author can be the same or not the same and if the author is the same, the label is 1, otherwise 0\n",
        "\n",
        "class TextPairDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, text2hash, hash2embedding, duplicate_factor = 1):\n",
        "        self.df = df\n",
        "        self.text2hash = text2hash\n",
        "        self.hash2embedding = hash2embedding\n",
        "        self.texts = df[\"text_hash\"].values\n",
        "        self.authors = df[\"author\"].values\n",
        "        self.duplicate_factor = duplicate_factor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)*self.duplicate_factor\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        idx = idx // self.duplicate_factor\n",
        "        sample_same = torch.randn(1).item() > 0\n",
        "        if sample_same:\n",
        "            author = self.authors[idx]\n",
        "            same_author_idx = np.where(self.authors == author)[0]\n",
        "            other_text_idx = torch.randint(0, len(same_author_idx), (1,)).item()\n",
        "            other_text_idx = same_author_idx[other_text_idx]\n",
        "        else:\n",
        "            other_text_idx = torch.randint(0, len(self.texts), (1,)).item()\n",
        "            iter = 0\n",
        "            while other_text_idx == idx or self.authors[idx] == self.authors[other_text_idx]:\n",
        "                other_text_idx = torch.randint(0, len(self.texts), (1,)).item()\n",
        "                iter += 1\n",
        "                if iter > 20:\n",
        "                    break\n",
        "\n",
        "        sample_same = self.authors[idx] == self.authors[other_text_idx]\n",
        "        assert (self.authors[idx] == self.authors[other_text_idx]) == sample_same, f\"author: {self.authors[idx]}, other author: {self.authors[other_text_idx]}, sample_same: {sample_same}\"\n",
        "\n",
        "        emb1 = self.hash2embedding[self.texts[idx]]\n",
        "        emb2 = self.hash2embedding[self.texts[other_text_idx]]\n",
        "\n",
        "        embedding_concat = torch.cat([emb1, emb2], dim=0)\n",
        "\n",
        "        return embedding_concat, sample_same\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyn8g4KWtLJu"
      },
      "outputs": [],
      "source": [
        "ds = TextPairDataset(train_df, text2hash, hash2embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miLuTpOjtLJw"
      },
      "outputs": [],
      "source": [
        "VAL_FRAC = 0.2\n",
        "val_df = train_df.iloc[:int(len(train_df) * VAL_FRAC)]\n",
        "train_df = train_df.iloc[int(len(train_df) * VAL_FRAC):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HYW51autLJx"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(TextPairDataset(train_df, text2hash, hash2embedding, duplicate_factor=100), batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(TextPairDataset(val_df, text2hash, hash2embedding, duplicate_factor=100), batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(TextPairDataset(test_df, text2hash, hash2embedding, duplicate_factor=100), batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OGs7edqtLJy",
        "outputId": "2ce92226-d927-4758-92ed-b7dff8e53883"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1667"
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ],
      "source": [
        "len(val_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPacHHcptLJz"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class One_Layer_MLP(torch.nn.Module):\n",
        "  def __init__(self,  n_input_units):\n",
        "\n",
        "    super(One_Layer_MLP, self).__init__()\n",
        "\n",
        "    self.n_input_units = n_input_units\n",
        "    self.fc1 = torch.nn.Linear(n_input_units, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = self.fc1(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class Linear_skip_block(nn.Module):\n",
        "  \"\"\"\n",
        "  Block of linear layer + softplus + skip connection +  dropout  + batchnorm\n",
        "  \"\"\"\n",
        "  def __init__(self, n_input, dropout_rate):\n",
        "    super(Linear_skip_block, self).__init__()\n",
        "\n",
        "    self.fc = nn.Linear(n_input, n_input)\n",
        "    self.act = torch.nn.LeakyReLU()\n",
        "\n",
        "    self.bn = nn.BatchNorm1d(n_input, affine = True)\n",
        "    self.drop = nn.Dropout(dropout_rate)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x0 = x\n",
        "    x = self.fc(x)\n",
        "    x = self.act(x)\n",
        "    x = x0 + x\n",
        "    x = self.drop(x)\n",
        "    x = self.bn(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class Linear_block(nn.Module):\n",
        "  \"\"\"\n",
        "  Block of linear layer dropout  + batchnorm\n",
        "  \"\"\"\n",
        "  def __init__(self, n_input, n_output, dropout_rate):\n",
        "    super(Linear_block, self).__init__()\n",
        "\n",
        "    self.fc = nn.Linear(n_input, n_output)\n",
        "    self.act = torch.nn.LeakyReLU()\n",
        "    self.bn = nn.BatchNorm1d(n_output, affine = True)\n",
        "    self.drop = nn.Dropout(dropout_rate)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc(x)\n",
        "    x = self.act(x)\n",
        "    x = self.drop(x)\n",
        "    x = self.bn(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, n_input_units, n_hidden_units, n_skip_layers, dropout_rate):\n",
        "\n",
        "    super(MLP, self).__init__()\n",
        "    self.n_input_units = n_input_units\n",
        "    self.n_hidden_units = n_hidden_units\n",
        "    self.n_skip_layers = n_skip_layers\n",
        "    self.dropout_rate = dropout_rate\n",
        "\n",
        "    self.linear1 = Linear_block(n_input_units, n_hidden_units, dropout_rate)    # initial linear layer\n",
        "    self.hidden_layers = torch.nn.Sequential(*[Linear_skip_block(n_hidden_units, dropout_rate) for _ in range(n_skip_layers)])  #hidden skip-layers\n",
        "\n",
        "    self.linear_final =  torch.nn.Linear(n_hidden_units, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear1(x)\n",
        "    x = self.hidden_layers(x)\n",
        "    x = self.linear_final(x)\n",
        "\n",
        "    return(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvyUZ2n5tLJ0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fB2uu8ltLJ0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "#Validation function\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef, precision_score, recall_score\n",
        "\n",
        "def validate(model, dataloader, loss_fun):\n",
        "    val_loss_lis = []\n",
        "\n",
        "    target_lis = []\n",
        "    pred_lis = []\n",
        "\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "\n",
        "          X, y = batch\n",
        "          X = X.to(device)\n",
        "          y = y.to(device)\n",
        "\n",
        "          pred = model(X)\n",
        "          pred = pred.squeeze(-1)\n",
        "          loss = loss_fun(pred, y.float())\n",
        "          val_loss_lis.append(loss.cpu().detach())\n",
        "\n",
        "          target_lis.append(y.detach().cpu())\n",
        "          pred_lis.append(pred.detach().cpu())\n",
        "\n",
        "    mean_loss = np.mean(np.array(val_loss_lis))\n",
        "    median_loss = np.median(np.array(val_loss_lis))\n",
        "\n",
        "    target_ten, pred_ten = torch.cat(target_lis), torch.cat(pred_lis)\n",
        "\n",
        "    pred_binary = (pred_ten > 0.5).float().cpu().detach().numpy()\n",
        "    acc = accuracy_score(pred_binary, target_ten)\n",
        "    f1 = f1_score(pred_binary, target_ten)\n",
        "    mcc = matthews_corrcoef(pred_binary, target_ten)\n",
        "    precision = precision_score(pred_binary, target_ten)\n",
        "    recall = recall_score(pred_binary, target_ten)\n",
        "    class_imabalance = np.mean(target_ten.cpu().detach().numpy())\n",
        "\n",
        "    return mean_loss, median_loss, acc, f1, mcc, precision, recall, class_imabalance\n",
        "\n",
        "\n",
        "\n",
        "# Training function\n",
        "def train_loop(model, optimizer, loss_fun, trainset, valset, print_mod, device, n_epochs, save_path = None, early_stopping = True, n_epochs_early_stopping = 5):\n",
        "    \"\"\"\n",
        "    train the model\n",
        "    Args:\n",
        "        model: The model to train\n",
        "        optimizer: The used optimizer\n",
        "        loss_fun: The used loss function\n",
        "        trainset: The dataset to train on\n",
        "        valset: The dataset to use for validation\n",
        "        print_mod: Number of epochs to print result after\n",
        "        device: Either \"cpu\" or \"cuda\"\n",
        "        n_epochs: Number of epochs to train\n",
        "        save_path: Path to save the model's state dict\n",
        "        config: config file from the model to train\n",
        "        sparse_ten (bool): if a sparse tensor is used for each batch\n",
        "    \"\"\"\n",
        "    if early_stopping == True:\n",
        "      n_early_stopping = n_epochs_early_stopping\n",
        "      past_val_losses = []\n",
        "\n",
        "    loss_lis = []\n",
        "    target_lis = []\n",
        "    pred_lis = []\n",
        "\n",
        "    loss_lis_all = []\n",
        "    val_loss_lis_all = []\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(n_epochs):\n",
        "      start = time.time()\n",
        "      for iter, batch in enumerate(tqdm(trainset)):\n",
        "\n",
        "        X, y = batch\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        pred = model(X)\n",
        "        pred = pred.squeeze(-1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        loss = loss_fun(pred, y.float())\n",
        "        #print(loss)\n",
        "\n",
        "        optimizer.zero_grad()       # clear previous gradients\n",
        "        loss.backward()             # backprop\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_lis.append(loss.cpu().detach())\n",
        "        target_lis.append(y.detach().cpu())\n",
        "        pred_lis.append(pred.detach().cpu())\n",
        "\n",
        "      if epoch % print_mod == 0:\n",
        "\n",
        "        end = time.time()\n",
        "        time_delta = end - start\n",
        "\n",
        "        mean_loss = np.mean(np.array(loss_lis))\n",
        "        median_loss = np.median(np.array(loss_lis))\n",
        "\n",
        "        target_ten, pred_ten = torch.cat(target_lis), torch.cat(pred_lis)\n",
        "\n",
        "        pred_binary = (pred_ten > 0.5).float().cpu().detach().numpy()\n",
        "        acc = accuracy_score(pred_binary, target_ten)\n",
        "        f1 = f1_score(pred_binary, target_ten)\n",
        "        mcc = matthews_corrcoef(pred_binary, target_ten)\n",
        "        precision = precision_score(pred_binary, target_ten)\n",
        "        recall = recall_score(pred_binary, target_ten)\n",
        "        class_imabalance = np.mean(target_ten.cpu().detach().numpy())\n",
        "\n",
        "        target_lis = []\n",
        "        pred_lis = []\n",
        "\n",
        "\n",
        "\n",
        "        loss_lis_all += loss_lis\n",
        "\n",
        "        loss_lis = []\n",
        "\n",
        "        mean_loss_val, median_loss_val, acc_val, f1_val, mcc_val, precision_val, recall_val, class_imabalance_val = validate(model, valset, loss_fun)\n",
        "\n",
        "        val_loss_lis_all.append(mean_loss_val)\n",
        "\n",
        "\n",
        "\n",
        "        print(f'Epoch nr {epoch}: mean_train_loss = {mean_loss}, median_train_loss = {median_loss}, train_acc = {acc}, train_f1 = {f1}, train_mcc = {mcc}, train_precision = {precision}, train_recall = {recall}, class_imbalance = {class_imabalance}, time = {time_delta}')\n",
        "        print(f'Epoch nr {epoch}: mean_valid_loss = {mean_loss_val}, median_valid_loss = {median_loss_val}, valid_acc = {acc_val}, valid_f1 = {f1_val}, valid_mcc = {mcc_val}, valid_precision = {precision_val}, valid_recall = {recall_val}, class_imbalance = {class_imabalance_val}, time = {time_delta}')\n",
        "\n",
        "\n",
        "\n",
        "        # early stopping based on median validation loss:\n",
        "        if early_stopping:\n",
        "          if len(past_val_losses) == 0 or mean_loss_val < min(past_val_losses):\n",
        "            print(\"save model\")\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "\n",
        "          if len(past_val_losses) >= n_early_stopping:\n",
        "            if mean_loss_val > max(past_val_losses):\n",
        "              print(f\"Early stopping because the median validation loss has not decreased since the last {n_early_stopping} epochs\")\n",
        "              return loss_lis_all, val_loss_lis_all\n",
        "            else:\n",
        "              past_val_losses = past_val_losses[1:] + [mean_loss_val]\n",
        "          else:\n",
        "            past_val_losses = past_val_losses + [mean_loss_val]\n",
        "\n",
        "\n",
        "\n",
        "    return loss_lis_all, val_loss_lis_all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJs9kgqptLJ1"
      },
      "outputs": [],
      "source": [
        "mlp1 = MLP(3072 , 768, 5, 0.3).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fE6RFJJttLJ1"
      },
      "outputs": [],
      "source": [
        "lr = 1e-4\n",
        "model = mlp1\n",
        "loss = torch.nn.BCEWithLogitsLoss()\n",
        "save_path = \"mlp1.pth\"\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDY9IcO3tLJ2",
        "outputId": "ce7d3566-3bd0-4256-ce63-fafc9ffcdcc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20838/20838 [03:10<00:00, 109.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch nr 0: mean_train_loss = 0.4904441833496094, median_train_loss = 0.47691333293914795, train_acc = 0.727128074385123, train_f1 = 0.6922557806376757, train_mcc = 0.46658802558300155, train_precision = 0.6134626337719956, train_recall = 0.7942720967510314, class_imbalance = 0.5002849430113977, time = 190.64577078819275\n",
            "Epoch nr 0: mean_valid_loss = 0.6683225035667419, median_valid_loss = 0.6578793525695801, valid_acc = 0.6260947810437912, valid_f1 = 0.5572587404639797, valid_mcc = 0.26471968732426826, valid_precision = 0.4713247221387804, valid_recall = 0.6815157148565769, class_imbalance = 0.499250149970006, time = 190.64577078819275\n",
            "save model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20838/20838 [03:04<00:00, 113.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch nr 1: mean_train_loss = 0.15463069081306458, median_train_loss = 0.1383957862854004, train_acc = 0.9330713857228554, train_f1 = 0.9326741754743795, train_mcc = 0.866207371235375, train_precision = 0.9269936185015474, train_recall = 0.9384247818774628, class_imbalance = 0.5000959808038392, time = 184.33452606201172\n",
            "Epoch nr 1: mean_valid_loss = 1.8623181581497192, median_valid_loss = 1.836830735206604, valid_acc = 0.5802999400119976, valid_f1 = 0.4223484535742003, valid_mcc = 0.1943123822649192, valid_precision = 0.30592301987895604, valid_recall = 0.6818351460865856, class_imbalance = 0.5015356928614277, time = 184.33452606201172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20838/20838 [03:01<00:00, 114.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch nr 2: mean_train_loss = 0.055339887738227844, median_train_loss = 0.0342259407043457, train_acc = 0.9786592681463707, train_f1 = 0.978678325911453, train_mcc = 0.9573189700434434, train_precision = 0.9782005085948943, train_recall = 0.9791566102507983, class_imbalance = 0.5006913617276545, time = 181.85622692108154\n",
            "Epoch nr 2: mean_valid_loss = 2.4933085441589355, median_valid_loss = 2.4781675338745117, valid_acc = 0.5835692861427715, valid_f1 = 0.42760906669744975, valid_mcc = 0.19748545606859846, valid_precision = 0.31181981071948245, valid_recall = 0.680184670269136, class_imbalance = 0.4988422315536893, time = 181.85622692108154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20838/20838 [03:02<00:00, 113.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch nr 3: mean_train_loss = 0.02567809261381626, median_train_loss = 0.009267398156225681, train_acc = 0.9899040191961608, train_f1 = 0.989850440236401, train_mcc = 0.9798111147782113, train_precision = 0.9885031858641581, train_recall = 0.9912013720303876, class_imbalance = 0.4980383923215357, time = 182.99867701530457\n",
            "Epoch nr 3: mean_valid_loss = 2.73236083984375, median_valid_loss = 2.7040822505950928, valid_acc = 0.5795800839832034, valid_f1 = 0.4195461321848601, valid_mcc = 0.1897564803184181, valid_precision = 0.30426216904521647, valid_recall = 0.6754853851077448, class_imbalance = 0.4993641271745651, time = 182.99867701530457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20838/20838 [03:01<00:00, 114.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch nr 4: mean_train_loss = 0.01773754507303238, median_train_loss = 0.004068843089044094, train_acc = 0.9930773845230954, train_f1 = 0.9930867362985283, train_mcc = 0.986156164671564, train_precision = 0.9922485956191097, train_recall = 0.9939262941100642, class_imbalance = 0.5010992801439712, time = 181.27217817306519\n",
            "Epoch nr 4: mean_valid_loss = 2.1499428749084473, median_valid_loss = 2.1217617988586426, valid_acc = 0.5925914817036593, valid_f1 = 0.4639657771568836, valid_mcc = 0.2112993831451685, valid_precision = 0.3525192798973338, valid_recall = 0.678454364987766, class_imbalance = 0.5001619676064787, time = 181.27217817306519\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20838/20838 [03:01<00:00, 114.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch nr 5: mean_train_loss = 0.013855164870619774, median_train_loss = 0.002224269090220332, train_acc = 0.9943266346730654, train_f1 = 0.9943190265307317, train_mcc = 0.9886542924267023, train_precision = 0.9935983433124747, train_recall = 0.9950407559691264, class_imbalance = 0.49969256148770247, time = 181.41322231292725\n",
            "Epoch nr 5: mean_valid_loss = 2.3861944675445557, median_valid_loss = 2.363478183746338, valid_acc = 0.5916076784643072, valid_f1 = 0.44873071784282764, valid_mcc = 0.21452788999602762, valid_precision = 0.33230592101318046, valid_recall = 0.6907314154659221, class_imbalance = 0.5001859628074385, time = 181.41322231292725\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20838/20838 [03:02<00:00, 114.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch nr 6: mean_train_loss = 0.01233094371855259, median_train_loss = 0.0015489619690924883, train_acc = 0.9949400119976005, train_f1 = 0.9949290304739118, train_mcc = 0.9898809484136571, train_precision = 0.9942326928218053, train_recall = 0.9956263442073124, class_imbalance = 0.4992666466706659, time = 182.19059205055237\n",
            "Epoch nr 6: mean_valid_loss = 2.636901617050171, median_valid_loss = 2.6124095916748047, valid_acc = 0.5781823635272946, valid_f1 = 0.44631847494862165, valid_mcc = 0.17814690943647435, valid_precision = 0.3398608945916777, valid_recall = 0.6498887844252333, class_imbalance = 0.500239952009598, time = 182.19059205055237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20838/20838 [03:02<00:00, 114.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch nr 7: mean_train_loss = 0.011124277487397194, median_train_loss = 0.001053161104209721, train_acc = 0.9953464307138572, train_f1 = 0.9953504819585814, train_mcc = 0.9906936254605698, train_precision = 0.9947319241205398, train_recall = 0.9959698095555096, class_imbalance = 0.500746850629874, time = 182.64097833633423\n",
            "Epoch nr 7: mean_valid_loss = 2.292752265930176, median_valid_loss = 2.275287628173828, valid_acc = 0.5935152969406119, valid_f1 = 0.4608836095441924, valid_mcc = 0.21405993614073038, valid_precision = 0.3478824857671335, valid_recall = 0.6826141264641418, class_imbalance = 0.4994481103779244, time = 182.64097833633423\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20838/20838 [03:01<00:00, 114.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch nr 8: mean_train_loss = 0.010614250786602497, median_train_loss = 0.0007939180359244347, train_acc = 0.9954484103179364, train_f1 = 0.9954497546465728, train_mcc = 0.9908978131853086, train_precision = 0.9947473137288979, train_recall = 0.9961531883227361, class_imbalance = 0.500500899820036, time = 181.45130348205566\n",
            "Epoch nr 8: mean_valid_loss = 2.3988454341888428, median_valid_loss = 2.369483232498169, valid_acc = 0.5801499700059988, valid_f1 = 0.42995023497886414, valid_mcc = 0.18681647923016317, valid_precision = 0.31742252047479885, valid_recall = 0.6660778276888911, class_imbalance = 0.49880623875224955, time = 181.45130348205566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20838/20838 [03:00<00:00, 115.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch nr 9: mean_train_loss = 0.009810848161578178, median_train_loss = 0.0006672918098047376, train_acc = 0.9958908218356328, train_f1 = 0.9958857252685539, train_mcc = 0.9917824593626013, train_precision = 0.9952431257915618, train_recall = 0.9965291550971836, class_imbalance = 0.4997030593881224, time = 180.82779335975647\n",
            "Epoch nr 9: mean_valid_loss = 3.2928004264831543, median_valid_loss = 3.261770725250244, valid_acc = 0.5731493701259748, valid_f1 = 0.44470977509325593, valid_mcc = 0.16502487138421407, valid_precision = 0.3418435291718156, valid_recall = 0.6361322587126879, class_imbalance = 0.50000599880024, time = 180.82779335975647\n",
            "Early stopping because the median validation loss has not decreased since the last 5 epochs\n"
          ]
        }
      ],
      "source": [
        "r = train_loop(model = model,\n",
        "           optimizer = opt,\n",
        "           loss_fun = loss,\n",
        "           trainset = train_loader,\n",
        "           valset = val_loader,\n",
        "           print_mod = 1,\n",
        "           device = device,\n",
        "           early_stopping = True,\n",
        "           n_epochs_early_stopping = 5,\n",
        "           save_path = \"mlp1.pth\",\n",
        "              n_epochs = 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufn6lFmmtLJ2",
        "outputId": "7da2052d-2447-4809-9f04-f4b148029122"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(save_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1P4y53-tLJ3",
        "outputId": "382707a9-2533-4adb-8420-7cce3e05d8a0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6250165,\n",
              " 0.6171717,\n",
              " 0.6445201535508637,\n",
              " 0.5661427098950524,\n",
              " 0.3089414812243957,\n",
              " 0.46480877785150354,\n",
              " 0.7239788505609395,\n",
              " 0.49898752399232243)"
            ]
          },
          "metadata": {},
          "execution_count": 183
        }
      ],
      "source": [
        "model = model.eval().to(device)\n",
        "\n",
        "validate(model, test_loader, loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVbNF6WatLJ3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}